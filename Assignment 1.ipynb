{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"biology.csv\")\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data from paragraph tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset['content'].values.tolist()\n",
    "paragraph_text= []\n",
    "for para in content:\n",
    "    soup = BeautifulSoup(para, 'html.parser')\n",
    "    paragraph_text.extend([p.getText() for p in soup.find_all('p')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lymphocytes may be as small as 6–9 μm in diameter or as large as 10–14 μm in diameter. \n",
      "\n",
      "Those ranges are quite close to each others. Should the above be taken to mean that lymphocytes sizes are clustered in two groups, or is it just a way of saying that lymphocytes are 6-14 μm? \n",
      "\n",
      "Various people in our lab will prepare a liter or so of LB, add kanamycin to 25-37 mg/L for selection, and store it at 4 °C for minipreps or other small cultures (where dosing straight LB with a 1000X stock is troublesome).  Some think using it after more than a week is dubious, but we routinely use kan plates that are 1-2 months old with no ill effect. \n",
      "\n",
      "How long can LB with antibiotic such as kanamycin, chloramphenicol, or ampicillin be stored at 4 °C and maintain selection? \n",
      "\n",
      "Are there any cases in which the splicing machinery constructs an mRNA in which the exons are not in the 5' -> 3' genomic order? I'm interested any such cases, whether they involve constitutive or alternative splicing. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in paragraph_text[5:10]:\n",
    "    print(p, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing unnecessary tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(paragraph_text)):\n",
    "    paragraph_text[i] = re.sub(r'<.*/?>', '', paragraph_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "for i in range(len(paragraph_text)):\n",
    "    paragraph_text[i] = expand_contractions(paragraph_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['In', 'prokaryotic', 'translation', 'how', 'critical', 'for', 'efficient', 'translation', 'is', 'the', 'location', 'of', 'the', 'ribosome', 'binding', 'site', 'relative', 'to', 'the', 'start', 'codon'], ['Ideally', 'it', 'is', 'supposed', 'to', 'be', '7b', 'away', 'from', 'the', 'start', 'How', 'about', 'if', 'it', 'is', '9', 'bases', 'away', 'or', 'even', 'more', 'Will', 'this', 'have', 'an', 'observable', 'effect', 'on', 'translation'], ['Does', 'anyone', 'have', 'any', 'suggestions', 'to', 'prevent', 'RNAse', 'contamination', 'when', 'working', 'with', 'RNA'], ['I', 'tend', 'to', 'have', 'issues', 'with', 'degradation', 'regardless', 'of', 'whether', 'I', 'use', 'DEPC', 'treated', 'RNAse', 'free', 'water', 'and', 'filtered', 'pipette', 'tips'], ['Tortora', 'writes', 'in', 'Principles', 'of', 'Anatomy', 'and', 'Physiology']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "for i in range(len(paragraph_text)):\n",
    "    paragraph_text[i] = tokenizer.tokenize(paragraph_text[i])\n",
    "\n",
    "print(paragraph_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covert Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'prokaryotic', 'translation', 'how', 'critical', 'for', 'efficient', 'translation', 'is', 'the', 'location', 'of', 'the', 'ribosome', 'binding', 'site', 'relative', 'to', 'the', 'start', 'codon', 'ideally', 'it', 'is', 'supposed', 'to', 'be', '7b', 'away', 'from', 'the', 'start', 'how', 'about', 'if', 'it', 'is', '9', 'bases', 'away', 'or', 'even', 'more', 'will', 'this']\n"
     ]
    }
   ],
   "source": [
    "paragraph_text = [word.lower() for sentence in paragraph_text for word in sentence]\n",
    "print(paragraph_text[:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1202364"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "paragraph_text = [w for w in paragraph_text if not w in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prokaryotic translation critical efficient translation location ribosome binding site relative start codon ideally supposed 7b away start 9 bases away even observable effect translation anyone suggestions prevent rnase contamination working rna tend issues degradation regardless whether use depc treated rnase free water filtered pipette tips "
     ]
    }
   ],
   "source": [
    "for word in paragraph_text[:45]:\n",
    "    print(word, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "628485"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hence we can see that length almost halved after removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmed_words = [] \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for word in paragraph_text:\n",
    "    stemmed_words.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmetized_words = []\n",
    "for word in paragraph_text:\n",
    "    lemmetized_words.append(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming vs Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot  :  plot  (Stemmed) :  plot  (Lemmatized)\n",
      "power  :  power  (Stemmed) :  power  (Lemmatized)\n",
      "frequencies  :  frequenc  (Stemmed) :  frequency  (Lemmatized)\n",
      "get  :  get  (Stemmed) :  get  (Lemmatized)\n",
      "following  :  follow  (Stemmed) :  following  (Lemmatized)\n"
     ]
    }
   ],
   "source": [
    "for i in range(628095, 628100):\n",
    "    print(paragraph_text[i], \" : \", stemmed_words[i], \" (Stemmed) : \", lemmetized_words[i], \" (Lemmatized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is obsered that sometimes Stemmer returns wrong root form of a word that isn't correct according to English Dictionary. While the Lemmatizer always returns the correct root form of a word.\n",
    "\n",
    "### Example: for the word 'frequencies', Stemmer returns 'frequenc' while Lemmatizer returns 'frequency'.\n",
    "\n",
    "### Stemmer is faster than Lemmatizer but less accurate. Choose which one to use according to need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import xml.etree.ElementTree as ET \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"business\")\n",
    "all_files = []\n",
    "for x in files:\n",
    "    if x[-4:] == \"utf8\":\n",
    "        all_files.append(x)\n",
    "files = all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = \"business/\"\n",
    "def getFileName(num):\n",
    "    return basePath+files[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing TF, IDF, TF-IDF Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, words):\n",
    "    tfDict = {}\n",
    "    wordCount = len(words)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(wordCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text using XML parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(xmlFilePath):\n",
    "    xmlFile = open(xmlFilePath)\n",
    "    tree = ET.parse(xmlFile)\n",
    "    root = tree.getroot()\n",
    "    return root[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_part1(text):\n",
    "    text = text.strip()\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (Stop word removal & Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_part2(text):\n",
    "    tokens = preprocess_part1(text)\n",
    "    text = [word.lower() for word in tokens]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = list(filter(lambda token: token not in stopwords.words(\"english\"),text))\n",
    "    tokens2 = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing five files by my approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common pre-processing as done in assignment 1\n",
    "text1 = preprocess_part2(getText(getFileName(0)))\n",
    "text2 = preprocess_part2(getText(getFileName(1)))\n",
    "text3 = preprocess_part2(getText(getFileName(2)))\n",
    "text4 = preprocess_part2(getText(getFileName(3)))\n",
    "text5 = preprocess_part2(getText(getFileName(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering unique words from all five files\n",
    "word_union = set(text1).union(text2).union(text3).union(text4).union(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDict1 = dict.fromkeys(word_union, 0)\n",
    "wordDict2 = dict.fromkeys(word_union, 0)\n",
    "wordDict3 = dict.fromkeys(word_union, 0)\n",
    "wordDict4 = dict.fromkeys(word_union, 0)\n",
    "wordDict5 = dict.fromkeys(word_union, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming Word Dictionary by counting Frequency of each word\n",
    "\n",
    "for word in text1:\n",
    "    wordDict1[word]+=1\n",
    "for word in text2:\n",
    "    wordDict2[word]+=1\n",
    "for word in text3:\n",
    "    wordDict3[word]+=1\n",
    "for word in text4:\n",
    "    wordDict4[word]+=1\n",
    "for word in text5:\n",
    "    wordDict5[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>various</th>\n",
       "      <th>exchange</th>\n",
       "      <th>lanka</th>\n",
       "      <th>anup</th>\n",
       "      <th>announced</th>\n",
       "      <th>malaysia</th>\n",
       "      <th>superb</th>\n",
       "      <th>entrepolis</th>\n",
       "      <th>banking</th>\n",
       "      <th>earlier</th>\n",
       "      <th>...</th>\n",
       "      <th>2007</th>\n",
       "      <th>slr</th>\n",
       "      <th>association</th>\n",
       "      <th>moving</th>\n",
       "      <th>model</th>\n",
       "      <th>generate</th>\n",
       "      <th>capability</th>\n",
       "      <th>9</th>\n",
       "      <th>procurement</th>\n",
       "      <th>minimise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 697 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   various  exchange  lanka  anup  announced  malaysia  superb  entrepolis  \\\n",
       "0        0         0      0     0          0         0       0           0   \n",
       "1        0         0      0     0          0         0       0           1   \n",
       "2        0         3      0     1          1         0       1           0   \n",
       "3        0         0      0     0          0         0       0           0   \n",
       "4        1         0      1     0          1         1       0           0   \n",
       "\n",
       "   banking  earlier  ...  2007  slr  association  moving  model  generate  \\\n",
       "0        0        0  ...     0    0            0       0      0         0   \n",
       "1        0        0  ...     1    1            0       0      1         0   \n",
       "2        0        1  ...     0    0            1       0      0         1   \n",
       "3        1        0  ...     0    0            0       0      0         0   \n",
       "4        0        3  ...     0    0            0       1      0         0   \n",
       "\n",
       "   capability  9  procurement  minimise  \n",
       "0           0  0            0         0  \n",
       "1           1  0            0         1  \n",
       "2           0  1            1         0  \n",
       "3           1  0            0         0  \n",
       "4           0  1            0         0  \n",
       "\n",
       "[5 rows x 697 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDict1, wordDict2, wordDict3, wordDict4, wordDict5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = computeTF(wordDict1, text1)\n",
    "tf2 = computeTF(wordDict2, text2)\n",
    "tf3 = computeTF(wordDict3, text3)\n",
    "tf4 = computeTF(wordDict4, text4)\n",
    "tf5 = computeTF(wordDict5, text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([wordDict1, wordDict2, wordDict3, wordDict4, wordDict5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = computeTFIDF(tf1, idfs)\n",
    "tfidf2 = computeTFIDF(tf2, idfs)\n",
    "tfidf3 = computeTFIDF(tf3, idfs)\n",
    "tfidf4 = computeTFIDF(tf4, idfs)\n",
    "tfidf5 = computeTFIDF(tf5, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>various</th>\n",
       "      <th>exchange</th>\n",
       "      <th>lanka</th>\n",
       "      <th>anup</th>\n",
       "      <th>announced</th>\n",
       "      <th>malaysia</th>\n",
       "      <th>superb</th>\n",
       "      <th>entrepolis</th>\n",
       "      <th>banking</th>\n",
       "      <th>earlier</th>\n",
       "      <th>...</th>\n",
       "      <th>2007</th>\n",
       "      <th>slr</th>\n",
       "      <th>association</th>\n",
       "      <th>moving</th>\n",
       "      <th>model</th>\n",
       "      <th>generate</th>\n",
       "      <th>capability</th>\n",
       "      <th>9</th>\n",
       "      <th>procurement</th>\n",
       "      <th>minimise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 697 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    various  exchange     lanka      anup  announced  malaysia    superb  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.004809  0.000000  0.001603   0.000913  0.000000  0.001603   \n",
       "3  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
       "4  0.002219  0.000000  0.002219  0.000000   0.001263  0.002219  0.000000   \n",
       "\n",
       "   entrepolis   banking   earlier  ...      2007       slr  association  \\\n",
       "0    0.000000  0.000000  0.000000  ...  0.000000  0.000000     0.000000   \n",
       "1    0.002589  0.000000  0.000000  ...  0.002589  0.002589     0.000000   \n",
       "2    0.000000  0.000000  0.000913  ...  0.000000  0.000000     0.001603   \n",
       "3    0.000000  0.003584  0.000000  ...  0.000000  0.000000     0.000000   \n",
       "4    0.000000  0.000000  0.003790  ...  0.000000  0.000000     0.000000   \n",
       "\n",
       "     moving     model  generate  capability         9  procurement  minimise  \n",
       "0  0.000000  0.000000  0.000000    0.000000  0.000000     0.000000  0.000000  \n",
       "1  0.000000  0.002589  0.000000    0.001474  0.000000     0.000000  0.002589  \n",
       "2  0.000000  0.000000  0.001603    0.000000  0.000913     0.001603  0.000000  \n",
       "3  0.000000  0.000000  0.000000    0.002041  0.000000     0.000000  0.000000  \n",
       "4  0.002219  0.000000  0.000000    0.000000  0.001263     0.000000  0.000000  \n",
       "\n",
       "[5 rows x 697 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tfidf1, tfidf2, tfidf3, tfidf4, tfidf5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFID-F Vectorization using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    str1 = \"\"  \n",
    "    for ele in s:  \n",
    "        str1 += (\" \"+ele)\n",
    "    return str1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = listToString(text1)\n",
    "t2 = listToString(text2)\n",
    "t3 = listToString(text3)\n",
    "t4 = listToString(text4)\n",
    "t5 = listToString(text5)\n",
    "list_raw_text = [t1, t2, t3, t4, t5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>05</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>1164</th>\n",
       "      <th>15</th>\n",
       "      <th>170</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>187</th>\n",
       "      <th>...</th>\n",
       "      <th>welcomed</th>\n",
       "      <th>well</th>\n",
       "      <th>west</th>\n",
       "      <th>witnessed</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yadav</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yogendra</th>\n",
       "      <th>yojna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.108393</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072592</td>\n",
       "      <td>0.054196</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.053792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.053792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.066978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.033337</td>\n",
       "      <td>0.033337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.099673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.026005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.03883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03883</td>\n",
       "      <td>0.03883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077659</td>\n",
       "      <td>0.03883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 687 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000        05        10        11      1164        15      170  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "1  0.000000  0.043725  0.000000  0.054196  0.000000  0.043725  0.00000   \n",
       "2  0.053792  0.000000  0.066978  0.000000  0.033337  0.000000  0.00000   \n",
       "3  0.099673  0.000000  0.041369  0.000000  0.000000  0.000000  0.00000   \n",
       "4  0.000000  0.031328  0.026005  0.000000  0.000000  0.031328  0.03883   \n",
       "\n",
       "        180      181      187  ...  welcomed      well      west  witnessed  \\\n",
       "0  0.000000  0.00000  0.00000  ...  0.000000  0.000000  0.000000    0.00000   \n",
       "1  0.108393  0.00000  0.00000  ...  0.000000  0.000000  0.000000    0.00000   \n",
       "2  0.000000  0.00000  0.00000  ...  0.033337  0.053792  0.000000    0.00000   \n",
       "3  0.000000  0.00000  0.00000  ...  0.000000  0.049836  0.000000    0.00000   \n",
       "4  0.000000  0.03883  0.03883  ...  0.000000  0.000000  0.077659    0.03883   \n",
       "\n",
       "      world     would     yadav  yesterday  yogendra     yojna  \n",
       "0  0.000000  0.000000  0.000000    0.00000  0.000000  0.000000  \n",
       "1  0.000000  0.072592  0.054196    0.00000  0.000000  0.000000  \n",
       "2  0.033337  0.066978  0.000000    0.00000  0.033337  0.033337  \n",
       "3  0.000000  0.000000  0.000000    0.00000  0.000000  0.000000  \n",
       "4  0.000000  0.052009  0.000000    0.03883  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 687 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(list_raw_text)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "dataframe = pd.DataFrame(denselist, columns=feature_names)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My approach vs sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My approach: Top 10 keywords by tf-idf score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bharat': 0.02440847634189272,\n",
       " 'petro': 0.01775161915774016,\n",
       " 'corporation': 0.01775161915774016,\n",
       " 'refinery': 0.011094761973587601,\n",
       " 'price': 0.011094761973587601,\n",
       " 'oil': 0.00887580957887008,\n",
       " 'kochi': 0.00887580957887008,\n",
       " 'merger': 0.00887580957887008,\n",
       " 'petroleum': 0.006656857184152561,\n",
       " 'behuria': 0.006656857184152561}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing keywords of 5th document\n",
    "\n",
    "import operator\n",
    "import itertools\n",
    "sorted_d = dict(sorted(tfidf5.items(), key=operator.itemgetter(1),reverse=True))\n",
    "print('My approach: Top 10 keywords by tf-idf score : ')\n",
    "out = dict(itertools.islice(sorted_d.items(), 10))  \n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn: Top 10 keywords by tf-idf score : \n",
      "\n",
      "{'bharat': 0.4271266676165389, 'corporation': 0.31063757644839196, 'petro': 0.31063757644839196, 'price': 0.19414848528024498, 'refinery': 0.19414848528024498, 'kochi': 0.15531878822419598, 'merger': 0.15531878822419598, 'oil': 0.15531878822419598, 'high': 0.12531022037266545, 'behuria': 0.11648909116814699}\n"
     ]
    }
   ],
   "source": [
    "rowData = dataframe.loc[ 4 , : ]\n",
    "d = rowData.to_dict()\n",
    "sorted_d = dict(sorted(d.items(), key=operator.itemgetter(1),reverse=True))\n",
    "print('Sklearn: Top 10 keywords by tf-idf score : \\n')\n",
    "out = dict(itertools.islice(sorted_d.items(), 10))  \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Conclusion: In both the approaches, 9 out of top 10 keywords are same although having different tf-idf scores. Thus we can safely say that both approaches are correct and can be used for ranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
